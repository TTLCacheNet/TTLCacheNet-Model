{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11996326,"sourceType":"datasetVersion","datasetId":7515257},{"sourceId":13544914,"sourceType":"datasetVersion","datasetId":8602115}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LSTM + GBDT for sequence-based prediction\n### Goal: Learn temporal patterns with LSTM, feed 64-D embeddings to GBDT for final prediction.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:58:45.548071Z","iopub.execute_input":"2025-11-03T12:58:45.548325Z","iopub.status.idle":"2025-11-03T12:58:47.463596Z","shell.execute_reply.started":"2025-11-03T12:58:45.548295Z","shell.execute_reply":"2025-11-03T12:58:47.462703Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deep-cache-dataset/y_dataset1_window.npy\n/kaggle/input/deep-cache-dataset/syntheticDataset_O50_properties.csv\n/kaggle/input/deep-cache-dataset/syntheticDataset_O50.csv\n/kaggle/input/deep-cache-dataset/X_dataset1_window.npy\n/kaggle/input/ttl-dataset/request_data.csv\n/kaggle/input/ttl-dataset/pred_lambda.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Use GPU if available, otherwise fall back to CPU\nimport tensorflow as tf\n\ndevice = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:58:47.465419Z","iopub.execute_input":"2025-11-03T12:58:47.465865Z","iopub.status.idle":"2025-11-03T12:59:04.023718Z","shell.execute_reply.started":"2025-11-03T12:58:47.465841Z","shell.execute_reply":"2025-11-03T12:59:04.022160Z"}},"outputs":[{"name":"stderr","text":"2025-11-03 12:58:49.197380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762174729.427179      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762174729.493110      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/CPU:0\n","output_type":"stream"},{"name":"stderr","text":"2025-11-03 12:59:04.018988: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load data\ndf = pd.read_csv('/kaggle/input/deep-cache-dataset/syntheticDataset_O50.csv') \ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.024728Z","iopub.execute_input":"2025-11-03T12:59:04.025666Z","iopub.status.idle":"2025-11-03T12:59:04.253073Z","shell.execute_reply.started":"2025-11-03T12:59:04.025640Z","shell.execute_reply":"2025-11-03T12:59:04.252176Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        object_ID   request_time\n0               5       2.961472\n1              25       3.274127\n2               2       3.785475\n3               2       4.455687\n4               4       5.288994\n...           ...            ...\n292141         39  304330.451276\n292142         39  304352.296649\n292143         39  304405.469075\n292144         39  304442.964190\n292145         39  304487.172906\n\n[292146 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>object_ID</th>\n      <th>request_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>2.961472</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>3.274127</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3.785475</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>4.455687</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5.288994</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>292141</th>\n      <td>39</td>\n      <td>304330.451276</td>\n    </tr>\n    <tr>\n      <th>292142</th>\n      <td>39</td>\n      <td>304352.296649</td>\n    </tr>\n    <tr>\n      <th>292143</th>\n      <td>39</td>\n      <td>304405.469075</td>\n    </tr>\n    <tr>\n      <th>292144</th>\n      <td>39</td>\n      <td>304442.964190</td>\n    </tr>\n    <tr>\n      <th>292145</th>\n      <td>39</td>\n      <td>304487.172906</td>\n    </tr>\n  </tbody>\n</table>\n<p>292146 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Split Data\n- To align with the paper's goal which is predicting the future characteristics of an object based on past logs, we split the dataset into 60% for training and 40% for evaluation.","metadata":{}},{"cell_type":"code","source":"train_cut = int(len(df) * 0.7)\nval_cut = int(len(df) * 0.85)\n\ntrain_df = df.iloc[:train_cut].reset_index(drop=True)\nval_df   = df.iloc[train_cut:val_cut].reset_index(drop=True)\ntest_df  = df.iloc[val_cut:].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.254014Z","iopub.execute_input":"2025-11-03T12:59:04.254345Z","iopub.status.idle":"2025-11-03T12:59:04.261345Z","shell.execute_reply.started":"2025-11-03T12:59:04.254318Z","shell.execute_reply":"2025-11-03T12:59:04.260489Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Feature Engineering\n> \"For dataset 1, the probability of object $o^i$ is calculated as $Ni$ /1000, where $N^i$ represents the number of occurrences of $o^i$ in the window of past 1K objects.\"\n- Using step=1 generated too much data, so we used step=100 to make it manageable.","metadata":{}},{"cell_type":"code","source":"window_size = 1000\nstep = 50\nm, k = 20, 10 # Sequence length for input(m) and output(k)\n\nobject_ids = df['object_ID'].unique()\nobject_ids.sort()\nnum_objects = len(object_ids) # Number of unique objects: 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.262289Z","iopub.execute_input":"2025-11-03T12:59:04.262598Z","iopub.status.idle":"2025-11-03T12:59:04.282074Z","shell.execute_reply.started":"2025-11-03T12:59:04.262568Z","shell.execute_reply":"2025-11-03T12:59:04.281057Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Generate training data for sequence-to-sequence modeling from object request logs\ndef build_sequence_input(df, object_ids, m, k, window_size, step):\n    X, y = [], []\n    for i in range(0, len(df) - window_size * (m + k), step):\n        seq = df['object_ID'].iloc[i : i + window_size * (m + k)]\n        x_seq, y_seq = [], []\n\n        # Build the input sequence: m windows of past requests\n        for j in range(m):\n            window = seq[j * window_size : (j + 1) * window_size]\n            counts = window.value_counts(normalize=True).reindex(object_ids, fill_value=0).values\n            x_seq.append(counts)\n\n        # Build the output sequence: k windows of future requests\n        for j in range(k):\n            window = seq[(m + j) * window_size : (m + j + 1) * window_size]\n            counts = window.value_counts(normalize=True).reindex(object_ids, fill_value=0).values\n            y_seq.append(counts)\n\n        X.append(x_seq)\n        y.append(y_seq)\n\n    X = np.array(X) # (#samples, 20, d)\n    y = np.array(y) # (#samples, 26, d)\n    X = X.transpose(0, 2, 1).reshape(-1, m, 1)\n    y = y.transpose(0, 2, 1).reshape(-1, k, 1)\n    return X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.283046Z","iopub.execute_input":"2025-11-03T12:59:04.283585Z","iopub.status.idle":"2025-11-03T12:59:04.292150Z","shell.execute_reply.started":"2025-11-03T12:59:04.283524Z","shell.execute_reply":"2025-11-03T12:59:04.291143Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef build_simple_sequences(df, object_ids, m, k, window_size, step, time_col=\"request_time\"):\n    \"\"\"\n    df: time-sorted DataFrame with [time_col, 'object_ID']\n    object_ids: unique object IDs (sorted)\n    return: X_seq (N, m, 1), y_pop (N, d), y_next (N, d)\n    \"\"\"\n    X_seq, y_pop, y_next = [], [], []\n\n    # 1) 시간 정렬\n    df = df.sort_values(time_col).reset_index(drop=True)\n\n    # 2) 시간 배열/객체 배열 만들기\n    ts = df[time_col].to_numpy().astype(float)  # seconds (float)\n    obj = df['object_ID'].to_numpy()\n    id2idx = {o: i for i, o in enumerate(object_ids)}\n    d = len(object_ids)\n\n    # 3) 슬라이딩 윈도우\n    total = window_size * (m + k)\n    for i in range(0, len(df) - total, step):\n        seq = obj[i : i + total]\n        ts_seq = ts[i : i + total]\n\n        # 입력: 과거 m개 창 (각 창 길이: window_size)\n        x_seq = []\n        for j in range(m):\n            w = seq[j*window_size : (j+1)*window_size]\n            ts_w  = ts_seq[j*window_size : (j+1)*window_size]\n            \n            counts = np.zeros(d, dtype=float)\n            unique, cnts = np.unique(w, return_counts=True)\n            for u, c in zip(unique, cnts):\n                counts[id2idx[u]] = c / window_size      # 비율\n\n            # 평균 간격 — 창 전체의 평균 간격을 각 객체 위치에 broadcast\n            if len(ts_w) >= 2:\n                gap_mean = float(np.mean(np.diff(ts_w)))\n            else:\n                gap_mean = float(window_size)\n            gap_mean = 0.0 if not np.isfinite(gap_mean) else gap_mean\n            gap_vec = np.full_like(counts, gap_mean, dtype=float)\n\n            # (d, 2)\n            x_seq.append(np.stack([counts, gap_vec], axis=1))\n            \n        # (m, d) -> (d, m, 1)\n        # X_seq.append(np.array(x_seq).T.reshape(-1, m, 1))\n        X_seq.append(np.stack(x_seq, axis=0).transpose(1, 0, 2))  # (d, m, 2)\n\n        # 미래 구간\n        future_objs = seq[m*window_size:]\n        future_ts   = ts_seq[m*window_size:]\n\n        # 인기도 라벨: 미래 구간에 한 번이라도 등장했는지\n        L = 3\n        future_subset = future_objs[:L * window_size]\n        y_pop.append(np.isin(object_ids, future_subset).astype(int))\n\n        \n        # y_pop.append(np.isin(object_ids, future_objs).astype(int))  # (d,)\n\n        # (b) 다음 inter-arrival: 기준시각 이후 첫 등장까지 시간\n        next_time = []\n        t_ref = ts_seq[m*window_size - 1]\n        for oid in object_ids:\n            mask = (future_objs == oid)\n            if np.any(mask):\n                idx_first = np.argmax(mask)  # True가 처음인 위치\n                next_time.append(max(0.0, float(future_ts[idx_first] - t_ref)))\n            else:\n                next_time.append(np.nan)     # 미래에 아예 안 나옴\n        y_next.append(next_time)\n\n    if not X_seq:\n        return (np.zeros((0, m, 1)), np.zeros((0, d), int), np.zeros((0, d), float))\n\n    X_seq = np.concatenate(X_seq, axis=0)   # (N, m, 1)\n    y_pop = np.array(y_pop, dtype=int)      # (N, d)\n    y_next = np.array(y_next, dtype=float)  # (N, d)\n    return X_seq, y_pop, y_next\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.294940Z","iopub.execute_input":"2025-11-03T12:59:04.295269Z","iopub.status.idle":"2025-11-03T12:59:04.318998Z","shell.execute_reply.started":"2025-11-03T12:59:04.295238Z","shell.execute_reply":"2025-11-03T12:59:04.317948Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Build LSTM Model\n> \"For our datasets, we use a two-layer depth LSTM Encoder-Decoder model with 128 and 64 as the number of hidden units. ... The loss function is chosen as mean-squared-error (MSE).\"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, TimeDistributed, RepeatVector\nfrom tensorflow.keras.models import Model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.320043Z","iopub.execute_input":"2025-11-03T12:59:04.320417Z","iopub.status.idle":"2025-11-03T12:59:04.415068Z","shell.execute_reply.started":"2025-11-03T12:59:04.320361Z","shell.execute_reply":"2025-11-03T12:59:04.414175Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# =========================\n# 1) LSTM 인코더 + 간단한 사전학습 헤드\n#    - 사전학습 타깃: next inter-arrival(또는 임의 예측 과제)\n#    - 학습 후 인코더만 떼어내 임베딩 추출에 사용\n# =========================\ndef build_encoder(m, head_units: int = 32):\n    # ----- Encoder -----\n    encoder_inputs = Input(shape=(m, 1), name=\"seq_in\")  # 이름을 seq_in으로 고정해도 OK\n\n    x = LSTM(128, return_sequences=True, name=\"enc_lstm_1\")(encoder_inputs)\n    x = Dropout(0.2, name=\"enc_do_1\")(x)\n    x = LSTM(64, name=\"enc_lstm_2\")(x)\n    emb = Dropout(0.2, name=\"emb_drop\")(x)  # embedding\n\n    # pretrain head: 다음 inter-arrival 회귀(양수)\n    h = Dense(head_units, activation=\"relu\", name=\"pre_head\")(emb)\n    next_hat = Dense(1, activation=\"linear\", name=\"next_hat\")(h)\n\n    # 사전학습용 모델 (인코더 + 헤드)\n    # pretrain_model = Model(encoder_inputs, next_hat, name=\"encoder_pretrain\")\n    # pretrain_model.compile(optimizer=\"adam\", loss=\"mae\")\n    pretrain_model = Model(encoder_inputs, next_hat, name=\"encoder_pretrain\")\n    pretrain_model.compile(optimizer=\"adam\", loss=tf.keras.losses.Huber())\n\n    # 인코더 단독 모델 (입력=encoder_inputs)\n    encoder = Model(encoder_inputs, emb, name=\"encoder_only\")\n    return encoder, pretrain_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.415942Z","iopub.execute_input":"2025-11-03T12:59:04.416181Z","iopub.status.idle":"2025-11-03T12:59:04.422895Z","shell.execute_reply.started":"2025-11-03T12:59:04.416162Z","shell.execute_reply":"2025-11-03T12:59:04.422044Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# =========================\n# 2) 인코더 사전학습\n# =========================\ndef pretrain_encoder(pretratin_model, X_seq_train, y_next_train, X_seq_val=None, y_next_val=None, epochs=20, batch_size=128):\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n    ] if X_seq_val is not None else []\n    hist = pretrain_model.fit(\n        X_seq_train, y_next_train,\n        validation_data=(X_seq_val, y_next_val) if X_seq_val is not None else None,\n        epochs=epochs, batch_size=batch_size, verbose=1, callbacks=callbacks\n    )\n    return hist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.424021Z","iopub.execute_input":"2025-11-03T12:59:04.424301Z","iopub.status.idle":"2025-11-03T12:59:04.445593Z","shell.execute_reply.started":"2025-11-03T12:59:04.424267Z","shell.execute_reply":"2025-11-03T12:59:04.444659Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# =========================\n# 3) 임베딩 추출\n# =========================\ndef extract_embeddings(encoder, X_seq):\n    return encoder.predict(X_seq, verbose=0)  # shape: (n_samples, 64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.446490Z","iopub.execute_input":"2025-11-03T12:59:04.446790Z","iopub.status.idle":"2025-11-03T12:59:04.461971Z","shell.execute_reply.started":"2025-11-03T12:59:04.446769Z","shell.execute_reply":"2025-11-03T12:59:04.461026Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# =========================\n# 4) 탭울러 전처리\n# =========================\ndef fit_transform_tab_scaler(X_tab_train, X_tab_val=None, X_tab_test=None):\n    scaler = StandardScaler()\n    X_tab_train_s = scaler.fit_transform(X_tab_train)\n    X_tab_val_s = scaler.transform(X_tab_val) if X_tab_val is not None else None\n    X_tab_test_s = scaler.transform(X_tab_test) if X_tab_test is not None else None\n    return scaler, X_tab_train_s, X_tab_val_s, X_tab_test_s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.462875Z","iopub.execute_input":"2025-11-03T12:59:04.463188Z","iopub.status.idle":"2025-11-03T12:59:04.483281Z","shell.execute_reply.started":"2025-11-03T12:59:04.463158Z","shell.execute_reply":"2025-11-03T12:59:04.482311Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# =========================\n# 5) 2단계 GBDT 학습\n# =========================\ndef train_gbdt_popularity(X_train, y_train, X_val, y_val, calibrate=True):\n    base = LGBMClassifier(\n        objective=\"binary\", learning_rate=0.05, num_leaves=64,\n        n_estimators=2000, subsample=0.8, colsample_bytree=0.8, random_state=SEED\n    )\n    if calibrate:\n        # AUC 최적화 모델에 확률 보정 적용\n        clf = CalibratedClassifierCV(base, method=\"isotonic\", cv=5)\n        clf.fit(X_train, y_train)\n        return clf\n    else:\n        base.fit(X_train, y_train,\n                 eval_set=[(X_val, y_val)],\n                 eval_metric=\"auc\",\n                 early_stopping_rounds=100,\n                 verbose=100)\n        return base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.484367Z","iopub.execute_input":"2025-11-03T12:59:04.484680Z","iopub.status.idle":"2025-11-03T12:59:04.501795Z","shell.execute_reply.started":"2025-11-03T12:59:04.484657Z","shell.execute_reply":"2025-11-03T12:59:04.500909Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_gbdt_next(X_train, y_train, X_val, y_val):\n    reg = LGBMRegressor(\n        objective=\"mae\", learning_rate=0.05, num_leaves=64,\n        n_estimators=2000, subsample=0.8, colsample_bytree=0.8, random_state=SEED\n    )\n    reg.fit(X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=\"l1\",\n            early_stopping_rounds=100,\n            verbose=100)\n    return reg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.502647Z","iopub.execute_input":"2025-11-03T12:59:04.502940Z","iopub.status.idle":"2025-11-03T12:59:04.519367Z","shell.execute_reply.started":"2025-11-03T12:59:04.502913Z","shell.execute_reply":"2025-11-03T12:59:04.518426Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train_gbdt_hazard_bins(X_train, y_train_bins, X_val, y_val_bins, calibrate=True):\n    \"\"\"\n    y_*_bins: shape (n_samples, H). 각 열이 하나의 임계 τ에 대한 0/1 라벨\n    반환: 리스트 또는 dict[bin_idx] = 모델\n    \"\"\"\n    H = y_train_bins.shape[1]\n    models = []\n    for j in range(H):\n        y_tr = y_train_bins[:, j]\n        y_va = y_val_bins[:, j]\n        model = train_gbdt_popularity(X_train, y_tr, X_val, y_va, calibrate=calibrate)\n        models.append(model)\n    return models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.520353Z","iopub.execute_input":"2025-11-03T12:59:04.520752Z","iopub.status.idle":"2025-11-03T12:59:04.536763Z","shell.execute_reply.started":"2025-11-03T12:59:04.520723Z","shell.execute_reply":"2025-11-03T12:59:04.535665Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# =========================\n# 6) 평가 함수\n# =========================\ndef evaluate_popularity(model, X, y_true):\n    # 확률 예측\n    if hasattr(model, \"predict_proba\"):\n        y_prob = model.predict_proba(X)[:, 1]\n    else:\n        y_prob = model.predict(X)\n        if y_prob.ndim > 1:  # LGBMClassifier raw pred\n            y_prob = y_prob[:, 1]\n    auc = roc_auc_score(y_true, y_prob)\n    ap = average_precision_score(y_true, y_prob)\n    return {\"AUC\": auc, \"PR-AUC\": ap}\n\ndef evaluate_next(model, X, y_true):\n    y_hat = model.predict(X)\n    mae = mean_absolute_error(y_true, y_hat)\n    return {\"MAE\": mae}\n\ndef evaluate_hazard(models, X, y_true_bins):\n    out = []\n    for j, mdl in enumerate(models):\n        if hasattr(mdl, \"predict_proba\"):\n            prob = mdl.predict_proba(X)[:, 1]\n        else:\n            prob = mdl.predict(X)\n            if prob.ndim > 1:\n                prob = prob[:, 1]\n        auc = roc_auc_score(y_true_bins[:, j], prob)\n        ap = average_precision_score(y_true_bins[:, j], prob)\n        out.append({\"bin\": j, \"AUC\": auc, \"PR-AUC\": ap})\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.537888Z","iopub.execute_input":"2025-11-03T12:59:04.538163Z","iopub.status.idle":"2025-11-03T12:59:04.561180Z","shell.execute_reply.started":"2025-11-03T12:59:04.538144Z","shell.execute_reply":"2025-11-03T12:59:04.560197Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# =========================\n# 7) 파이프라인 묶기\n# =========================\ndef train_pipeline(\n    X_seq_train, X_seq_val, X_tab_train, X_tab_val,\n    y_pop_train, y_pop_val, y_next_train, y_next_val,\n    y_haz_train, y_haz_val,\n    use_log1p_for_next=True\n):\n    m = X_seq_train.shape[1]\n\n    # encoder + pretrain\n    encoder, pretrain_model = build_encoder(m)\n    y_next_tr = np.log1p(y_next_train) if use_log1p_for_next else y_next_train\n    y_next_va = np.log1p(y_next_val) if use_log1p_for_next else y_next_val\n    pretrain_encoder(pretrain_model, X_seq_train, y_next_tr, X_seq_val, y_next_va,\n                     epochs=20, batch_size=128)\n\n    # extract embedding\n    emb_tr = extract_embeddings(encoder, X_seq_train)   # (n, 64)\n    emb_va = extract_embeddings(encoder, X_seq_val)\n\n    # tabular scaling\n    scaler, X_tab_tr_s, X_tab_va_s, _ = fit_transform_tab_scaler(X_tab_train, X_tab_val, None)\n\n    X_tr = np.concatenate([emb_tr, X_tab_tr_s], axis=1)\n    X_va = np.concatenate([emb_va, X_tab_va_s], axis=1)\n\n    # train GBDT\n    mdl_pop = train_gbdt_popularity(X_tr, y_pop_train, X_va, y_pop_val, calibrate=True)\n\n    y_next_tr_target = np.log1p(y_next_train) if use_log1p_for_next else y_next_train\n    y_next_va_target = np.log1p(y_next_val) if use_log1p_for_next else y_next_val\n    mdl_next = train_gbdt_next(X_tr, y_next_tr_target, X_va, y_next_va_target)\n\n    mdl_haz_bins = train_gbdt_hazard_bins(X_tr, y_haz_train, X_va, y_haz_val, calibrate=True)\n\n    artifacts = {\n        \"encoder\": encoder,\n        \"scaler\": scaler,\n        \"mdl_pop\": mdl_pop,\n        \"mdl_next\": mdl_next,\n        \"mdl_haz_bins\": mdl_haz_bins\n    }\n    return artifacts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.562120Z","iopub.execute_input":"2025-11-03T12:59:04.562430Z","iopub.status.idle":"2025-11-03T12:59:04.579072Z","shell.execute_reply.started":"2025-11-03T12:59:04.562384Z","shell.execute_reply":"2025-11-03T12:59:04.578239Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# =========================\n# 8) 추론 래퍼\n# =========================\ndef predict_signals(artifacts, X_seq, X_tab, use_log1p_for_next=True):\n    encoder = artifacts[\"encoder\"]\n    scaler  = artifacts[\"scaler\"]\n    mdl_pop = artifacts[\"mdl_pop\"]\n    mdl_next = artifacts[\"mdl_next\"]\n    mdl_haz_bins = artifacts[\"mdl_haz_bins\"]\n\n    emb = extract_embeddings(encoder, X_seq)\n    X_tab_s = scaler.transform(X_tab)\n    X_ = np.concatenate([emb, X_tab_s], axis=1)\n\n    # popularity\n    if hasattr(mdl_pop, \"predict_proba\"):\n        pop_prob = mdl_pop.predict_proba(X_)[:, 1]\n    else:\n        pop_prob = mdl_pop.predict(X_)  # 보정 안 쓴 경우\n        if pop_prob.ndim > 1:\n            pop_prob = pop_prob[:, 1]\n\n    # next inter-arrival\n    next_hat = mdl_next.predict(X_)\n    if use_log1p_for_next:\n        next_hat = np.expm1(next_hat)\n\n    # hazard bins\n    haz_probs = []\n    for mdl in mdl_haz_bins:\n        if hasattr(mdl, \"predict_proba\"):\n            haz_probs.append(mdl.predict_proba(X_)[:, 1])\n        else:\n            p = mdl.predict(X_)\n            if p.ndim > 1:\n                p = p[:, 1]\n            haz_probs.append(p)\n    haz_probs = np.stack(haz_probs, axis=1)  # (n, H)\n\n    return {\n        \"popularity\": pop_prob,            # (n,)\n        \"next_interarrival_sec\": next_hat, # (n,)\n        \"hazard_proxy\": haz_probs          # (n, H)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.579945Z","iopub.execute_input":"2025-11-03T12:59:04.580217Z","iopub.status.idle":"2025-11-03T12:59:04.599290Z","shell.execute_reply.started":"2025-11-03T12:59:04.580190Z","shell.execute_reply":"2025-11-03T12:59:04.598341Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\nX_seq_train, y_pop_train, y_next_train = build_simple_sequences(train_df, object_ids, m, k, window_size, step)\nX_seq_val,   y_pop_val,   y_next_val   = build_simple_sequences(val_df, object_ids, m, k, window_size, step)\nX_seq_test,  y_pop_test,  y_next_test  = build_simple_sequences(test_df, object_ids, m, k, window_size, step)\n\n# 1) 시퀀스/라벨 생성\nX_seq_train, y_pop_train, y_next_train = build_simple_sequences(\n    train_df, object_ids, m, k, window_size, step, time_col=\"request_time\"\n)\nX_seq_val,   y_pop_val,   y_next_val   = build_simple_sequences(\n    val_df, object_ids, m, k, window_size, step, time_col=\"request_time\"\n)\n\n# 2) 라벨을 X에 맞춰 \"객체별\"로 펼치기 (flatten)\n#    y_next: (num_windows, d) -> (num_windows*d, 1)\ny_next_train_flat = y_next_train.reshape(-1, 1)\ny_next_val_flat   = y_next_val.reshape(-1, 1)\n\n\n\n# 3) NaN/inf 제거 (미래에 안 나오는 객체는 next_time이 NaN일 수 있음)\ndef drop_nan_rows(X_seq, y_flat):\n    mask = np.isfinite(y_flat[:, 0])\n    return X_seq[mask], y_flat[mask]\n\nX_seq_train_fit, y_next_train_fit = drop_nan_rows(X_seq_train, y_next_train_flat)\nX_seq_val_fit,   y_next_val_fit   = drop_nan_rows(X_seq_val,   y_next_val_flat)\n\nprint(\"X_seq_train_fit:\", X_seq_train_fit.shape)  # (N*d_filtered, m, 1)\nprint(\"y_next_train_fit:\", y_next_train_fit.shape)  # (N*d_filtered, 1)\n\n# 4) 인코더/사전학습\nencoder, pretrain_model = build_encoder(m=20)\nbatch_size = max(32, int(len(X_seq_train_fit) * 0.1))\n\nhistory = pretrain_model.fit(\n    X_seq_train_fit, y_next_train_fit,\n    validation_data=(X_seq_val_fit, y_next_val_fit),\n    epochs=20, batch_size=batch_size, verbose=1\n)\n\n# 5) 임베딩 추출 (val/test에도 동일)\nemb_train = encoder.predict(X_seq_train, verbose=0)\nemb_val   = encoder.predict(X_seq_val,   verbose=0)\n\n# 탭울러 피처가 있다면 concat해서 GBDT로 학습\n# X_gbdt_train = np.concatenate([emb_train, X_tab_train], axis=1)\n# X_gbdt_val   = np.concatenate([emb_val,   X_tab_val],   axis=1)\n\n# 임베딩만 GBDT 입력으로 사용\nX_gbdt_train = emb_train\nX_gbdt_val   = emb_val\n\n\n\n# Reshape X and y to fit LSTM input requirements:\n# X: (samples * num_objects, m, 1)\n# y: (samples * num_objects, K, 1)\nprint(\"X_seq_train:\", X_seq_train.shape)\nprint(\"y_pop_train:\", y_pop_train.shape, \"y_next_train:\", y_next_train.shape)\nprint(\"X_seq_val:  \", X_seq_val.shape)\nprint(\"X_seq_test: \", X_seq_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:04.600225Z","iopub.execute_input":"2025-11-03T12:59:04.600497Z","iopub.status.idle":"2025-11-03T12:59:29.538371Z","shell.execute_reply.started":"2025-11-03T12:59:04.600477Z","shell.execute_reply":"2025-11-03T12:59:29.536266Z"}},"outputs":[{"name":"stdout","text":"X_seq_train_fit: (174080, 20, 2)\ny_next_train_fit: (174080, 1)\nEpoch 1/20\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2741103404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_seq_train_fit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m history = pretrain_model.fit(\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mX_seq_train_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_next_train_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_seq_val_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_next_val_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 2 and 1 for '{{node encoder_pretrain_1/enc_lstm_1_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](encoder_pretrain_1/enc_lstm_1_1/strided_slice_1, encoder_pretrain_1/enc_lstm_1_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [17408,2], [1,512].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(17408, 2), dtype=float32)\n  • states=('tf.Tensor(shape=(17408, 128), dtype=float32)', 'tf.Tensor(shape=(17408, 128), dtype=float32)')\n  • training=True"],"ename":"ValueError","evalue":"Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 2 and 1 for '{{node encoder_pretrain_1/enc_lstm_1_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](encoder_pretrain_1/enc_lstm_1_1/strided_slice_1, encoder_pretrain_1/enc_lstm_1_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [17408,2], [1,512].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(17408, 2), dtype=float32)\n  • states=('tf.Tensor(shape=(17408, 128), dtype=float32)', 'tf.Tensor(shape=(17408, 128), dtype=float32)')\n  • training=True","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"np.save(\"X_seq_train.npy\", X_seq_train)\nnp.save(\"y_pop_train.npy\", y_pop_train)\nnp.save(\"y_next_train.npy\", y_next_train)\n\nnp.save(\"X_seq_val.npy\", X_seq_val)\nnp.save(\"y_pop_val.npy\", y_pop_val)\nnp.save(\"y_next_val.npy\", y_next_val)\n\nnp.save(\"X_seq_test.npy\", X_seq_test)\nnp.save(\"y_pop_test.npy\", y_pop_test)\nnp.save(\"y_next_test.npy\", y_next_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.538970Z","iopub.status.idle":"2025-11-03T12:59:29.539288Z","shell.execute_reply.started":"2025-11-03T12:59:29.539138Z","shell.execute_reply":"2025-11-03T12:59:29.539152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LSTM 인코더를 사전학습한 결과\n- 세로축은 MAE, 가로축은 Epoch\n- 학습이 진행될수록 검증 손실이 꾸준히 감소\n- -> 모델이 객체의 요청 간격 패턴을 안정적으로 학습하고 있음","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.title(\"Pretrain Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss (MAE)\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.540335Z","iopub.status.idle":"2025-11-03T12:59:29.540682Z","shell.execute_reply.started":"2025-11-03T12:59:29.540525Z","shell.execute_reply":"2025-11-03T12:59:29.540541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) 라벨을 객체별로 평탄화\ny_next_train_flat = y_next_train.reshape(-1)  # (N*d,)\ny_next_val_flat   = y_next_val.reshape(-1)\n\n# 2) 다음간격 회귀용 마스크 (NaN/inf 제거)\nmask_tr_next = np.isfinite(y_next_train_flat)\nmask_va_next = np.isfinite(y_next_val_flat)\n\n# 3) 마스크 적용한 시퀀스만으로 임베딩 추출\nX_seq_train_next = X_seq_train[mask_tr_next]      # (n_tr_fit, m, 1)\nX_seq_val_next   = X_seq_val[mask_va_next]        # (n_va_fit, m, 1)\n\nemb_train_next = encoder.predict(X_seq_train_next, verbose=0)  # (n_tr_fit, 64)\nemb_val_next   = encoder.predict(X_seq_val_next,   verbose=0)  # (n_va_fit, 64)\n\n# 4) 회귀용 y도 동일 마스크 적용 + log1p 변환\ny_next_train_fit = y_next_train_flat[mask_tr_next]             # (n_tr_fit,)\ny_next_val_fit   = y_next_val_flat[mask_va_next]               # (n_va_fit,)\ny_next_tr_t = np.log1p(y_next_train_fit)\ny_next_va_t = np.log1p(y_next_val_fit)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.541759Z","iopub.status.idle":"2025-11-03T12:59:29.542070Z","shell.execute_reply.started":"2025-11-03T12:59:29.541895Z","shell.execute_reply":"2025-11-03T12:59:29.541910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import early_stopping, log_evaluation\n\n# reg = lgb.LGBMRegressor(\n#     objective=\"mae\", learning_rate=0.05, num_leaves=64, n_estimators=2000\n# )\nreg = lgb.LGBMRegressor(\n    objective=\"quantile\", alpha=0.5,\n    learning_rate=0.05, num_leaves=64, n_estimators=2000\n)\nreg.fit(\n    emb_train_next, y_next_tr_t,\n    eval_set=[(emb_val_next, y_next_va_t)],\n    eval_metric=\"l1\",\n    callbacks=[early_stopping(100), log_evaluation(100)],\n)\n\n# 6) 예측 및 역변환\ny_next_val_pred = np.expm1(reg.predict(emb_val_next, num_iteration=reg.best_iteration_))\n\n# 7) 확인 (길이 일치해야 함)\nprint(\"emb_train_next:\", emb_train_next.shape,\n      \"y_next_tr_t:\", y_next_tr_t.shape)\nprint(\"emb_val_next:\", emb_val_next.shape,\n      \"y_next_va_t:\", y_next_va_t.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.542855Z","iopub.status.idle":"2025-11-03T12:59:29.543120Z","shell.execute_reply.started":"2025-11-03T12:59:29.542987Z","shell.execute_reply":"2025-11-03T12:59:29.542998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef expand_for_next(emb, ynext_mat, add_object_onehot=True):\n    \"\"\"\n    emb        : np.ndarray, (N_seq, F) 또는 (N_seq*d, F)\n    ynext_mat  : np.ndarray, (N_seq, d)   각 (시퀀스, 객체) 다음 도착까지 시간(초), 미등장은 NaN\n    add_object_onehot : bool, True면 객체 원-핫을 피처 뒤에 추가\n\n    return:\n      X_next : np.ndarray, (N_seq*d_filtered, F [+ d])\n      y_next : np.ndarray, (N_seq*d_filtered,)   # NaN 제거됨\n      obj_idx: np.ndarray, (N_seq*d_filtered,)   # 각 행의 객체 인덱스(0..d-1)\n    \"\"\"\n    emb = np.asarray(emb)\n    ynext_mat = np.asarray(ynext_mat)\n    assert emb.ndim == 2 and ynext_mat.ndim == 2, \"emb는 2D, ynext_mat는 2D여야 합니다.\"\n\n    N_seq, d = ynext_mat.shape\n    N_emb, F = emb.shape\n\n    # 공통: 평탄화 타깃과 객체 인덱스\n    y_flat = ynext_mat.reshape(-1)                         # (N_seq*d,)\n    obj_idx_all = np.tile(np.arange(d, dtype=np.int32), reps=N_seq)  # (N_seq*d,)\n\n    if N_emb == N_seq:\n        # 1) 임베딩이 시퀀스 단위 → 객체축으로 확장\n        X_rep = np.repeat(emb, repeats=d, axis=0)          # (N_seq*d, F)\n        X_base = X_rep\n    elif N_emb == N_seq * d:\n        # 2) 임베딩이 이미 시퀀스×객체 단위\n        X_base = emb                                       # (N_seq*d, F)\n    else:\n        raise ValueError(\n            f\"Shape mismatch: emb={emb.shape}, ynext_mat={ynext_mat.shape}. \"\n            \"emb의 첫 차원이 N_seq 또는 N_seq*d 여야 합니다.\"\n        )\n\n    # NaN(미등장) 제거 마스크\n    mask = ~np.isnan(y_flat)\n    X_base = X_base[mask]\n    y_next = y_flat[mask]\n    obj_idx = obj_idx_all[mask]\n\n    # 원-핫 추가 옵션\n    if add_object_onehot:\n        # 주의: d가 클 경우 메모리 사용량이 커질 수 있습니다.\n        obj_oh = np.eye(d, dtype=np.float32)[obj_idx]      # (n_kept, d)\n        X_next = np.concatenate([X_base.astype(np.float32, copy=False), obj_oh], axis=1)\n    else:\n        X_next = X_base.astype(np.float32, copy=False)\n\n    return X_next, y_next, obj_idx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.544187Z","iopub.status.idle":"2025-11-03T12:59:29.544461Z","shell.execute_reply.started":"2025-11-03T12:59:29.544317Z","shell.execute_reply":"2025-11-03T12:59:29.544327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef align_for_pop(emb, y_pop_mat, add_object_onehot=True):\n    \"\"\"\n    emb: (N_emb, F)\n    y_pop_mat: (N_seq, d)\n    return:\n      X_pop, y_pop, obj_idx\n    \"\"\"\n    N_emb, F = emb.shape\n    N_seq, d = y_pop_mat.shape\n\n    if N_emb == N_seq:\n        # 임베딩이 '시퀀스 단위' → 객체축으로 확장 필요\n        X_pop, y_pop, obj_idx = expand_objectwise_features(\n            emb, y_pop_mat, add_object_onehot=add_object_onehot\n        )\n        return X_pop, y_pop, obj_idx\n\n    elif N_emb == N_seq * d:\n        # 임베딩이 이미 '시퀀스×객체 단위' → 라벨만 평탄화\n        X_pop = emb\n        y_pop = y_pop_mat.reshape(-1).astype(np.int32, copy=False)\n        obj_idx = np.tile(np.arange(d, dtype=np.int32), reps=N_seq)\n        return X_pop, y_pop, obj_idx\n\n    else:\n        raise ValueError(\n            f\"Shape mismatch: emb={emb.shape}, y_pop_mat={y_pop_mat.shape}. \"\n            \"임베딩이 시퀀스 단위인지(=N_seq) 또는 시퀀스×객체 단위인지(=N_seq*d) 확인하세요.\"\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.545610Z","iopub.status.idle":"2025-11-03T12:59:29.545918Z","shell.execute_reply.started":"2025-11-03T12:59:29.545760Z","shell.execute_reply":"2025-11-03T12:59:29.545776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract embedding\nemb_tr = extract_embeddings(encoder, X_seq_train)   # (n, 64)\nemb_va = extract_embeddings(encoder, X_seq_val)\n\n# 객체 축으로 확장 (인기 분류용)\n# X_pop_tr, y_pop_tr, obj_idx_pop_tr = expand_objectwise_features(emb_tr, y_pop_train, add_object_onehot=True)\n# X_pop_va, y_pop_va, obj_idx_pop_va = expand_objectwise_features(emb_va, y_pop_val, add_object_onehot=True)\n\n# 2) 모양에 맞춰 자동 정렬\nX_pop_tr, y_pop_tr, obj_idx_pop_tr = align_for_pop(emb_tr, y_pop_train, add_object_onehot=True)\nX_pop_va, y_pop_va, obj_idx_pop_va = align_for_pop(emb_va, y_pop_val,   add_object_onehot=True)\n\n# 3) 학습\nclf = lgb.LGBMClassifier(objective=\"binary\", learning_rate=0.05, num_leaves=64, n_estimators=2000)\nclf.fit(\n    X_pop_tr, y_pop_tr,\n    eval_set=[(X_pop_va, y_pop_va)],\n    eval_metric=\"auc\",\n    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]\n)\n\n# next 회귀 입력 준비 (원-핫을 쓰지 않을 거면 False로 통일!)\nX_next_tr, y_next_tr, obj_idx_tr = expand_for_next(emb_tr, y_next_train, add_object_onehot=True)\nX_next_va, y_next_va, obj_idx_va = expand_for_next(emb_va, y_next_val, add_object_onehot=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.546660Z","iopub.status.idle":"2025-11-03T12:59:29.546886Z","shell.execute_reply.started":"2025-11-03T12:59:29.546775Z","shell.execute_reply":"2025-11-03T12:59:29.546785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#    λ̂ = 1 / (τ̂ + ε)  로 변환 후, object_id 별 중앙값으로 집계\nimport pandas as pd\neps = 1e-6\nhazard_va = 1.0 / (y_next_val_pred + eps)  # (N_va * d,)  각 (시퀀스,객체) 행의 λ̂\n\ndf_lambda_rows = pd.DataFrame({\n    \"object_id\": object_ids[obj_idx_va],   # obj_idx → 실제 object_id 매핑\n    \"pred_lambda\": hazard_va                    # 초당 요청률(1/초)\n})\n\n# 객체별 중앙값(robust)으로 대표값 산출\ndf_lambda = (\n    df_lambda_rows\n    .groupby(\"object_id\", as_index=False)[\"pred_lambda\"]\n    .median()\n    .sort_values(\"object_id\")\n)\n\n# CSV 저장\ndf_lambda.to_csv(\"pred_lambda.csv\", index=False)\nprint(\"Saved pred_lambda.csv\")\nprint(df_lambda.head())\n\nartifacts = {\n    \"encoder\": encoder,\n    \"reg_next\": reg,\n    \"clf_pop\": clf,\n    \"object_ids\": object_ids,\n    \"m\": m\n}\nprint(\"artifacts\", artifacts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.547913Z","iopub.status.idle":"2025-11-03T12:59:29.548448Z","shell.execute_reply.started":"2025-11-03T12:59:29.548287Z","shell.execute_reply":"2025-11-03T12:59:29.548301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir('.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.548969Z","iopub.status.idle":"2025-11-03T12:59:29.549835Z","shell.execute_reply.started":"2025-11-03T12:59:29.549674Z","shell.execute_reply":"2025-11-03T12:59:29.549691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, roc_auc_score, average_precision_score\n\nprint(\"Next MAE:\", mean_absolute_error(y_next_val_fit, y_next_val_pred))\n\nuniq = np.unique(y_pop_val_fit)\nif uniq.size < 2:\n    pos_rate = float((y_pop_val_fit == 1).mean())\n    print(f\"Pop AUC: (스킵, 한 클래스만 존재)  양성비율={pos_rate:.4f}\")\nelse:\n    print(\"Pop AUC:\", roc_auc_score(y_pop_val_fit, y_pop_val_prob))\n    print(\"Pop PR-AUC:\", average_precision_score(y_pop_val_fit, y_pop_val_prob))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.550872Z","iopub.status.idle":"2025-11-03T12:59:29.551171Z","shell.execute_reply.started":"2025-11-03T12:59:29.551045Z","shell.execute_reply":"2025-11-03T12:59:29.551059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = np.random.choice(len(y_next_val_fit), size=200, replace=False)\nplt.scatter(y_next_val_fit[idx], y_next_val_pred[idx], s=8)\nplt.xlabel(\"True next (sec)\"); plt.ylabel(\"Pred next (sec)\"); plt.title(\"Prediction vs Truth\"); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.552126Z","iopub.status.idle":"2025-11-03T12:59:29.552493Z","shell.execute_reply.started":"2025-11-03T12:59:29.552298Z","shell.execute_reply":"2025-11-03T12:59:29.552313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- X축: 실제 다음 도착 간격\n- Y축: 모델이 예측한 다음 도착 간격\n\n### 결과 해석\n- 대부분의 점이 (0~200초) 근처에 밀집되어 있음 → 대부분의 객체는 짧은 간격 내에 재등장\n- y축이 전반적으로 x축보다 낮게 분포 → 모델이 실제보다 짧은 inter-arrival time을 예측하는 경향\n","metadata":{}},{"cell_type":"code","source":"plt.hist(y_next_train_fit, bins=100)\nplt.yscale('log')\nplt.xlabel(\"next_interarrival (sec)\")\nplt.title(\"Label distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.553669Z","iopub.status.idle":"2025-11-03T12:59:29.553967Z","shell.execute_reply.started":"2025-11-03T12:59:29.553823Z","shell.execute_reply":"2025-11-03T12:59:29.553838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train = y_next_train_fit.copy()\ny_val   = y_next_val_fit.copy()\n\n# 상/하한 설정\nlow = np.percentile(y_train[np.isfinite(y_train)],  0.5)   # 필요시 0 또는 0.1초\nhigh = np.percentile(y_train[np.isfinite(y_train)], 99.0)  # 97~99.5% 사이로 테스트\n\ny_tr_t = np.log1p(np.clip(y_train, low, high))\ny_va_t = np.log1p(np.clip(y_val,   low, high))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.554819Z","iopub.status.idle":"2025-11-03T12:59:29.555119Z","shell.execute_reply.started":"2025-11-03T12:59:29.554953Z","shell.execute_reply":"2025-11-03T12:59:29.554967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_hat = np.expm1(reg.predict(emb_val_next))\nmae_lin = mean_absolute_error(y_val, y_hat)\nmae_log = mean_absolute_error(np.log1p(y_val), np.log1p(y_hat))\nprint({\"MAE\": mae_lin, \"MAE_log\": mae_log})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.557130Z","iopub.status.idle":"2025-11-03T12:59:29.557503Z","shell.execute_reply.started":"2025-11-03T12:59:29.557307Z","shell.execute_reply":"2025-11-03T12:59:29.557322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bins = [0, 30, 60, 120, 300, 600, 1200, 3600, 1e9]\nfor a,b in zip(bins[:-1], bins[1:]):\n    m = (y_val>=a)&(y_val<b)\n    if m.any():\n        print(f\"[{a},{b}) n={m.sum():4d}\",\n              \"MAE=\", mean_absolute_error(y_val[m], y_hat[m]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.558736Z","iopub.status.idle":"2025-11-03T12:59:29.559033Z","shell.execute_reply.started":"2025-11-03T12:59:29.558900Z","shell.execute_reply":"2025-11-03T12:59:29.558915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reg = lgb.LGBMRegressor(\n    objective=\"regression\",   # 로그라벨 회귀\n    learning_rate=0.05,\n    num_leaves=31,            # 과적합 완화\n    min_data_in_leaf=200,     # 데이터 크기에 맞춰 100~500 시도\n    n_estimators=4000,\n    subsample=0.8, colsample_bytree=0.8,\n)\nreg.fit(\n    emb_train_next, np.log1p(np.clip(y_train, low, high)),\n    eval_set=[(emb_val_next, np.log1p(np.clip(y_val, low, high)))],\n    eval_metric=\"l1\",\n    callbacks=[early_stopping(200), log_evaluation(200)],\n)\ny_hat = np.expm1(reg.predict(emb_val_next, num_iteration=reg.best_iteration_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.560039Z","iopub.status.idle":"2025-11-03T12:59:29.560311Z","shell.execute_reply.started":"2025-11-03T12:59:29.560183Z","shell.execute_reply":"2025-11-03T12:59:29.560193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.scatter(np.log1p(y_val), np.log1p(y_hat), s=8)\nplt.xlabel(\"log1p(True)\"); plt.ylabel(\"log1p(Pred)\"); plt.title(\"Log-Log Pred vs True\"); plt.show()\n\nimport pandas as pd\ndf = pd.DataFrame({\"y\":y_val, \"yhat\":y_hat})\ndf[\"bin\"] = pd.qcut(np.log1p(df[\"y\"]), q=10)\nprint(df.groupby(\"bin\").apply(lambda g: pd.Series({\n    \"n\": len(g),\n    \"true_med\": g[\"y\"].median(),\n    \"pred_med\": g[\"yhat\"].median(),\n    \"MAE\": mean_absolute_error(g[\"y\"], g[\"yhat\"])\n})))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.561264Z","iopub.status.idle":"2025-11-03T12:59:29.561588Z","shell.execute_reply.started":"2025-11-03T12:59:29.561447Z","shell.execute_reply":"2025-11-03T12:59:29.561463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TTL 추천 시스템\n\n### input 파일 4가지\n\n- feature_list.json : 필수! 모델에서 필요한 input 데이터 정의하는 파일\n>[\n  \"pred_lambda\",\n  \"lambda_mean\",\n  \"lambda_peak\",\n  \"lambda_cv\",\n  \"burst_score\",\n  \"n_req\",\n  \"log_size\",\n  \"hour_sin\",\n  \"hour_cos\",\n  \"wday_sin\",\n  \"wday_cos\",\n  \"tau_base\"\n]\n\n- features_df : feature_list의 데이터들의 실제 입력 데이터 (지금 sample_df로 임의로 값 넣어주고 있음)\n\n- base_policy.json : (선택) scale, size_coef, eps 값인데 기본값이 있어서 파일 안넣어도됨\n\n- model.joblib : (선택) 잔차 보정 모델인데 이것도 안넣어도 기본 휴리스틱으로 결과값 나옴\n\n- meta.json : (선택) tau_min, tau_max 값인데 기본값이 0.0, 300.0으로 설정되어있어서 파일 안넣어도 됨","metadata":{}},{"cell_type":"code","source":"# ttl_predictor_residual.py\nfrom __future__ import annotations\nimport math\nfrom datetime import datetime, timezone, timedelta\nfrom typing import Optional, Tuple\nimport json, joblib, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom typing import Tuple\n\nKST = timezone(timedelta(hours=9))\n\n# ==== Defaults & helpers ====\nDEFAULT_FEATURE_LIST = [\n    \"pred_lambda\",\"lambda_mean\",\"lambda_peak\",\"lambda_cv\",\"burst_score\",\"n_req\",\n    \"log_size\",\"hour_sin\",\"hour_cos\",\"wday_sin\",\"wday_cos\",\"tau_base\"\n]\nDEFAULT_BASE_POLICY = {\"scale\": 30.0, \"size_coef\": 0.3, \"eps\": 1e-6}\nDEFAULT_META = {\"tau_min\": 0.0, \"tau_max\": 300.0}\n\ndef _load_json_or_default(path: str | Path | None, default_obj):\n    try:\n        if path and Path(path).exists():\n            return json.loads(Path(path).read_text(encoding=\"utf-8\"))\n    except Exception:\n        pass\n    return default_obj\n\nclass ZeroModel:\n    def predict(self, X):\n        return np.zeros((len(X),), dtype=float)\n\n\ndef _validate_request_df(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"요청 로그 DataFrame 검증/정리: 필수 컬럼, 타입 캐스팅, NaN 제거 등.\"\"\"\n    required = {\"object_id\", \"request_time\", \"size_bytes\"}\n    if not required.issubset(df.columns):\n        raise ValueError(f\"request_df에는 {sorted(required)} 컬럼이 필요합니다. 현재 컬럼: {sorted(df.columns)}\")\n\n    df = df.copy()\n    # object_id는 float로 올 수 있으므로 문자열로 통일(그룹핑/머지 안정성)\n    df[\"object_id\"] = df[\"object_id\"].astype(str)\n    df[\"request_time\"] = pd.to_numeric(df[\"request_time\"], errors=\"coerce\")\n    df[\"size_bytes\"] = pd.to_numeric(df[\"size_bytes\"], errors=\"coerce\").fillna(0).astype(int)\n    df = df.dropna(subset=[\"request_time\"])\n    return df\n\ndef _compute_features_per_object(g: pd.DataFrame) -> dict:\n    \"\"\"단일 object_id 그룹에 대해 통계 피처 계산.\"\"\"\n    ts = np.sort(g[\"request_time\"].values.astype(float))\n    n = len(ts)\n\n    # 간격\n    if n >= 2:\n        intervals = np.diff(ts)\n        intervals = intervals[intervals > 0]  # 0 간격 제거\n    else:\n        intervals = np.array([], dtype=float)\n\n    # lambda_mean / lambda_peak\n    if intervals.size > 0:\n        lam_mean = 1.0 / np.mean(intervals)\n        lam_peak = 1.0 / np.min(intervals)\n    else:\n        lam_mean = 0.0\n        lam_peak = 0.0\n\n    # lambda_cv\n    if intervals.size >= 2 and np.mean(intervals) > 0:\n        lam_cv = float(np.std(intervals) / np.mean(intervals))\n    else:\n        lam_cv = 0.0\n\n    # burst_score (P95 / median)\n    if intervals.size >= 2 and np.median(intervals) > 0:\n        p95 = float(np.percentile(intervals, 95))\n        med = float(np.median(intervals))\n        burst = float(p95 / med) if med > 0 else 1.0\n        if not np.isfinite(burst) or burst <= 0:\n            burst = 1.0\n    else:\n        burst = 1.0\n\n    # n_req\n    n_req = int(n)\n\n    # size_bytes: 동일 object 내 상이할 수 있으므로 최대값 사용(정책에 따라 변경 가능)\n    size_val = int(np.max(g[\"size_bytes\"].values)) if n > 0 else 0\n\n    # 시간 피처: 가장 최신 요청 시각 기준, KST(+09:00)\n    last_ts = float(ts[-1]) if n > 0 else float(\"nan\")\n    if math.isfinite(last_ts):\n        dt_kst = datetime.fromtimestamp(last_ts, tz=KST)\n        hour = dt_kst.hour\n        weekday = dt_kst.weekday()  # Monday=0\n    else:\n        hour, weekday = 0, 0\n\n    return {\n        \"lambda_mean\": float(lam_mean),\n        \"lambda_peak\": float(lam_peak),\n        \"lambda_cv\": float(lam_cv),\n        \"burst_score\": float(burst),\n        \"n_req\": n_req,\n        \"size_bytes\": size_val,\n        \"hour\": int(hour),\n        \"weekday\": int(weekday),\n    }\n\ndef build_features_from_dataframe(\n    request_df: pd.DataFrame,\n    pred_lambda_df: Optional[pd.DataFrame] = None,\n) -> pd.DataFrame:\n    \"\"\"\n    요청 로그 DataFrame(필수)와 pred_lambda DataFrame(선택)을 받아 features_df 생성.\n    - request_df: columns=['object_id', 'request_time', 'size_bytes']\n    - pred_lambda_df: columns=['object_id', 'pred_lambda']  (없으면 pred_lambda=lambda_mean)\n    반환: features_df (object_id별 1행)\n    \"\"\"\n    req = _validate_request_df(request_df)\n\n    feats = (\n        req.sort_values([\"object_id\", \"request_time\"])\n           .groupby(\"object_id\", as_index=False)\n            .apply(lambda g: pd.Series(_compute_features_per_object(g)), include_groups=False)\n            .reset_index()\n           .drop(columns=[\"level_0\"], errors=\"ignore\")\n    )\n\n    if pred_lambda_df is not None:\n        pl = pred_lambda_df.copy()\n        if not {\"object_id\", \"pred_lambda\"}.issubset(pl.columns):\n            raise ValueError(\"pred_lambda_df에는 columns=['object_id','pred_lambda'] 가 필요합니다.\")\n        pl[\"object_id\"] = pl[\"object_id\"].astype(str)\n        pl[\"pred_lambda\"] = pd.to_numeric(pl[\"pred_lambda\"], errors=\"coerce\")\n        feats = feats.merge(pl[[\"object_id\", \"pred_lambda\"]], on=\"object_id\", how=\"left\")\n        feats[\"pred_lambda\"] = feats[\"pred_lambda\"].fillna(feats[\"lambda_mean\"])\n    else:\n        feats[\"pred_lambda\"] = feats[\"lambda_mean\"]\n\n    return feats\n\ndef build_features_from_csv(\n    request_csv_path: str,\n    pred_lambda_csv_path: Optional[str] = None,\n) -> Tuple[pd.DataFrame, str]:\n    \"\"\"\n    CSV를 직접 받아 features_df 생성.\n    반환: (features_df, pred_lambda_source)\n    \"\"\"\n    request_df = pd.read_csv('/kaggle/input/ttl-dataset/request_data.csv')\n    pred_df = None\n    src = \"lambda_mean(default)\"\n    if pred_lambda_csv_path:\n        pred_df = pd.read_csv('/kaggle/input/ttl-dataset/pred_lambda.csv')\n        src = \"pred_lambda.csv\"\n\n    feats = build_features_from_dataframe(request_df, pred_df)\n    return feats, src\n# ===== end of Data Preparation functions =====\n\n\n# -------- Guardrail defaults --------\n_DEFAULT_TAU_MIN = 0.0\n_DEFAULT_TAU_MAX = 300.0\n_EPS_RATE = 1e-6\n\ndef _cyclical_encode(series: pd.Series, period: float) -> Tuple[pd.Series, pd.Series]:\n    s = pd.to_numeric(series, errors=\"coerce\").fillna(0.0).astype(float)\n    return np.sin(2*np.pi*s/period), np.cos(2*np.pi*s/period)\n\ndef _safe_num(series: pd.Series, default: float = 0.0, min_value: float | None = None) -> np.ndarray:\n    \"\"\"Coerce to finite float, replace NaN/inf with default, clamp to min_value if provided.\"\"\"\n    x = pd.to_numeric(series, errors=\"coerce\").astype(float)\n    x = x.replace([np.inf, -np.inf], np.nan).fillna(default).to_numpy()\n    if min_value is not None:\n        x = np.maximum(x, min_value)\n    return x\n\ndef _compute_tau_base(df: pd.DataFrame, cfg: dict, tau_min: float, tau_max: float) -> np.ndarray:\n    \"\"\"\n    Heuristic baseline (updated to match policy):\n      - TTL increases with popularity (rate), decreases with size.\n      - tau_base = scale * rate / (1 + size_coef * log1p(size))\n      - Fallback: if BOTH pred_lambda and lambda_mean are missing -> return tau_max (default).\n      - Final clipping to [tau_min, tau_max].\n    \"\"\"\n    eps = float(cfg.get(\"eps\", _EPS_RATE))\n    scale = float(cfg.get(\"scale\", 30.0))\n    size_coef = float(cfg.get(\"size_coef\", 0.3))\n\n    # raw series to detect missing rows\n    rate_pred_raw = df.get(\"pred_lambda\", pd.Series([np.nan]*len(df)))\n    rate_obs_raw  = df.get(\"lambda_mean\", pd.Series([np.nan]*len(df)))\n    rate_missing_mask = rate_pred_raw.isna() & rate_obs_raw.isna()\n\n    # numeric, non-negative\n    rate_pred = _safe_num(rate_pred_raw, default=0.0, min_value=0.0)\n    rate_obs  = _safe_num(rate_obs_raw,  default=0.0, min_value=0.0)\n\n    # prefer predicted if available/nonzero else observed\n    rate = np.where(np.isfinite(rate_pred) & (rate_pred > 0.0), rate_pred, rate_obs)\n    rate = np.maximum(rate, 0.0)\n\n    size = _safe_num(df.get(\"size_bytes\", pd.Series([0]*len(df))), default=0.0, min_value=0.0)\n    log_size = np.log1p(size)\n\n    # proportional to rate (NOT inverse)\n    denom = (1.0 + size_coef * log_size)\n    tau = scale * np.maximum(eps, rate) / denom\n\n    # 원본 결측 플래그가 있으면 fallback -> default = tau_max\n    if \"_missing_rate\" in df.columns:\n        mask = df[\"_missing_rate\"].to_numpy(dtype=bool)\n        tau[mask] = tau_max\n    else:\n        # 레거시: 둘 다 NaN인 경우만 폴백\n        tau[rate_missing_mask.to_numpy()] = tau_max\n\n    return np.clip(tau, tau_min, tau_max)\n\n\n\n\nclass TTLResidualPredictor:\n    \"\"\"\n    Residual policy: TTL = tau_base + residual_model(features)\n    Guardrails are enforced here (clipping to [tau_min, tau_max], fallback defaults, numeric sanitation).\n    \"\"\"\n    def __init__(self, model_path: str | None = None,\n                 feature_list_path: str | None = None,\n                 base_policy_path: str | None = None,\n                 meta_path: str | None = None):\n\n        # 1) feature list / base policy / meta: 경로가 없거나 파일이 없어도 내부 디폴트 사용\n        self.feat_cols = _load_json_or_default(feature_list_path, DEFAULT_FEATURE_LIST)\n        self.base_cfg  = _load_json_or_default(base_policy_path, DEFAULT_BASE_POLICY)\n        meta = _load_json_or_default(meta_path, DEFAULT_META)\n        self.tau_min = float(meta.get(\"tau_min\", DEFAULT_META[\"tau_min\"]))\n        self.tau_max = float(meta.get(\"tau_max\", DEFAULT_META[\"tau_max\"]))\n\n        # 2) residual model: 모델 경로가 없거나 로드 실패 → ZeroModel 폴백\n        self.model = None\n        if model_path and Path(model_path).exists():\n            try:\n                self.model = joblib.load(model_path)\n            except Exception as e:\n                import warnings\n                warnings.warn(f\"[warn] failed to load model '{model_path}': {e}. Falling back to ZeroModel().\")\n        if self.model is None:\n            self.model = ZeroModel()\n\n    def _ensure_derived(self, df: pd.DataFrame) -> pd.DataFrame:\n        df = df.copy()\n\n        # '원본' 결측 여부 캡처\n        pred_raw = df.get(\"pred_lambda\")\n        obs_raw  = df.get(\"lambda_mean\") if \"lambda_mean\" in df else None\n        if pred_raw is None:\n            pred_missing = pd.Series([True]*len(df))\n        else:\n            pred_missing = pd.to_numeric(pred_raw, errors=\"coerce\").isna()\n        if obs_raw is None:\n            obs_missing = pd.Series([True]*len(df))\n        else:\n            obs_missing = pd.to_numeric(obs_raw, errors=\"coerce\").isna()\n        df[\"_missing_rate\"] = (pred_missing & obs_missing)\n\n\n        # Fill obvious missing columns with safe defaults\n        for col, dflt in [\n            (\"pred_lambda\", 0.0),\n            (\"lambda_mean\", 0.0),\n            (\"size_bytes\",  0.0),\n            (\"hour\",        0.0),\n            (\"weekday\",     0.0),\n            (\"lambda_peak\", 0.0),\n            (\"lambda_cv\",   0.0),\n            (\"burst_score\", 1.0),\n            (\"n_req\",       0.0),\n        ]:\n            if col not in df:\n                df[col] = dflt\n\n        # Derived features\n        hs, hc = _cyclical_encode(df.get(\"hour\"), 24.0)\n        df[\"hour_sin\"], df[\"hour_cos\"] = hs, hc\n        ws, wc = _cyclical_encode(df.get(\"weekday\"), 7.0)\n        df[\"wday_sin\"], df[\"wday_cos\"] = ws, wc\n        df[\"log_size\"] = np.log1p(_safe_num(df[\"size_bytes\"], default=0.0, min_value=0.0))\n        if \"lambda_x_size\" in self.feat_cols and \"lambda_x_size\" not in df:\n            df[\"lambda_x_size\"] = _safe_num(df[\"pred_lambda\"], 0.0, 0.0) * df[\"log_size\"]\n\n        # tau_base (with guardrails)\n        df[\"tau_base\"] = _compute_tau_base(df, self.base_cfg, self.tau_min, self.tau_max)\n\n        # Ensure all features exist\n        for c in self.feat_cols:\n            if c not in df:\n                df[c] = 0.0\n\n        # Sanitize final matrix\n        X = df[self.feat_cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(0.0).to_numpy(dtype=float)\n        return df, X\n\n    def predict_ttl(self, features_df: pd.DataFrame, prev_ttl: float | None = None,\n                    ema_ratio: float = 0.5, change_bound: float = 0.5) -> np.ndarray:\n        \"\"\"\n        Returns TTL seconds with guardrails & optional smoothing.\n        - Always clipped to [tau_min, tau_max]\n        - If model outputs NaN/inf, treated as 0 residual\n        - Optional smoothing & change bound applied if prev_ttl is given\n        \"\"\"\n        df, X = self._ensure_derived(features_df)\n        residual = self.model.predict(X)\n        # Replace any NaN/inf residual with 0\n        residual = np.where(np.isfinite(residual), residual, 0.0)\n\n        ttl = df[\"tau_base\"].to_numpy(dtype=float) + residual\n        ttl = np.clip(ttl, self.tau_min, self.tau_max)\n\n        if prev_ttl is not None:\n            low, high = (1.0 - change_bound) * prev_ttl, (1.0 + change_bound) * prev_ttl\n            ttl = np.minimum(np.maximum(ema_ratio * prev_ttl + (1 - ema_ratio) * ttl, low), high)\n\n        # Final safety: ensure finite & within bounds\n        ttl = np.where(np.isfinite(ttl), ttl, self.tau_min)\n        ttl = np.clip(ttl, self.tau_min, self.tau_max)\n        return ttl\n\nif __name__ == \"__main__\":\n    import os, json, numpy as np, pandas as pd, joblib\n    from pathlib import Path\n\n    # === Kaggle 경로 설정 ===\n    BASE_DIR = Path(\"/kaggle/input/ttl-dataset\")\n\n    REQUEST_CSV = BASE_DIR / \"request_data.csv\"\n    PRED_LAMBDA_CSV = BASE_DIR / \"pred_lambda.csv\"  # 없으면 자동 대체\n    FEATURES_CSV = \"features_df.csv\"\n    TTL_CSV = \"ttl_results.csv\"\n\n    MODEL_PATH = BASE_DIR / \"residual_zero_model.joblib\"\n    FEATURE_LIST_PATH = BASE_DIR / \"feature_list.json\"\n    BASE_POLICY_PATH = BASE_DIR / \"base_policy.json\"\n    META_PATH = BASE_DIR / \"meta.json\"\n\n    # === 입력 체크 ===\n    if not REQUEST_CSV.exists():\n        raise FileNotFoundError(f\"[ERROR] '{REQUEST_CSV}' 파일이 없습니다.\")\n\n    # 1) features 생성\n    pred_csv_path = PRED_LAMBDA_CSV if PRED_LAMBDA_CSV.exists() else None\n    feats, src_pred = build_features_from_csv(str(REQUEST_CSV), str(pred_csv_path) if pred_csv_path else None)\n    feats.to_csv(FEATURES_CSV, index=False)\n    print(f\"[saved] {FEATURES_CSV}  (pred_lambda source: {src_pred})\")\n\n    # 2) TTL 예측\n    predictor = TTLResidualPredictor(\n        model_path=str(MODEL_PATH) if MODEL_PATH.exists() else None,\n        feature_list_path=str(FEATURE_LIST_PATH) if FEATURE_LIST_PATH.exists() else None,\n        base_policy_path=str(BASE_POLICY_PATH) if BASE_POLICY_PATH.exists() else None,\n        meta_path=str(META_PATH) if META_PATH.exists() else None,\n    )\n\n    ttl = predictor.predict_ttl(feats)\n\n    out = pd.DataFrame({\n        \"object_id\": feats[\"object_id\"],\n        \"ttl_seconds\": ttl\n    })\n    out.to_csv(TTL_CSV, index=False)\n    print(f\"[saved] {TTL_CSV} (columns: object_id, ttl_seconds)\")\n\n    print(\"\\n=== TTL 예측 샘플(상위 10개) ===\")\n    print(out.head(10).to_string(index=False))\n    print(\"\\n완료\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T12:59:29.563253Z","iopub.status.idle":"2025-11-03T12:59:29.563588Z","shell.execute_reply.started":"2025-11-03T12:59:29.563431Z","shell.execute_reply":"2025-11-03T12:59:29.563448Z"}},"outputs":[],"execution_count":null}]}